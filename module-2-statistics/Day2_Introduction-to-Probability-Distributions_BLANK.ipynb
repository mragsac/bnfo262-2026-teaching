{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8fac16",
   "metadata": {},
   "source": [
    "# CMM 262: Introduction to Statistics, Day 2/2 (Distributions and Hypothesis Testing)\n",
    "\n",
    "## Notebook Information\n",
    "\n",
    "Now that we have some experience in data visualization and generating summary statistics, we'll continue with learning about different probability distributions and hypothesis testing. Again, I've re-adapted this lesson in **Python** instead of R for this year, and you can find the 2025 R programming language iteration from Graham [here](https://github.com/biom262/bnfo262-2025/blob/main/module-2-statistics/Day2_Distributions_CentralLimit_HypothesisTesting.Rmd).\n",
    "\n",
    "### Contributors\n",
    "\n",
    "* **Clarence Mah**, Ph.D. (CMM 262, 2020-2021)\n",
    "* **Michelle Franc Ragsac**, Ph.D. (CMM 262, 2020-2021, 2026)\n",
    "* **Graham McVicker**, Ph.D. (CMM 262, 2022-2024)\n",
    "\n",
    "## Dataset Background\n",
    "\n",
    "We'll be going through the **Palmer Penguins** dataset from [Dr. Kristen Gorman](https://www.uaf.edu/cfos/people/faculty/detail/kristen-gorman.php) and the [Palmer Station, Antarctica LTER](https://pal.lternet.edu/) again in this Jupyter Notebook. We'll be using the [upload somebody made to GitHub Gists](https://gist.github.com/slopp/ce3b90b9168f2f921784de84fa445651) that I put in the `data/` directory for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6207f79d",
   "metadata": {},
   "source": [
    "## Notebook Dependencies\n",
    "\n",
    "In addition to the packages that we imported in the previous lesson, we'll be importing the `scipy` and `numpy` packages. While you may be familiar with these two packages, I provided a brief background of what they are typically used for below:\n",
    "\n",
    "> The [`numpy`](https://numpy.org/) package adds support for large, multi-dimensional arrays and matrices to Python, along with many high-level mathematical functions to perform computations on those arrays.\n",
    ">\n",
    "> The [`scipy`](https://scipy.org/) package contains modules for optimization, linear algebra, integration, and statistics! In this notebook, we'll primarily be using the `scipy.stats` module for statistical computations.\n",
    "\n",
    "### Importing Packages of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2558df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# The new additions for this week!\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf4857c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee9837a",
   "metadata": {},
   "source": [
    "## Exercise 1: Discrete Distributions\n",
    "\n",
    "### Plotting the Probability Mass Function (PMF) for the Binomial Distribution\n",
    "\n",
    "The `scipy` package's `stats` module has many functions for working with various types of probability distributions. Depending on what you're working on, I highly suggest [reading the documentation](https://docs.scipy.org/doc/scipy/reference/stats.html) to see if there's support for the particular calculations you're interested in.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info: Limitations to the <code>scipy.stats</code> Module</b>\n",
    "    <p></p>\n",
    "    <p>While <code>scipy.stats</code> is powerful and has implementations for many different types of calculations for various probability distributions, the package is limited in some respects, which they mention on their website. Fortunately, they provide recommendations for other packages to use depending on the type of calculations you're interested in:</p>\n",
    "    <ul>\n",
    "        <li><a href=\"https://www.statsmodels.org/stable/index.html\">While <code>scipy.stats</code> has functionality for regression modeling, <code>statsmodels</code></a> is another Python package that is better suited for more advanced regression models, linear models, and time series analysis. <code>statsmodels</code> has some overlap with the functionalities offered by <code>scipy.stats</code>, but takes things a step further, while also mirroring more of the syntax used in the R programming language!</li>\n",
    "        <li><a href=\"https://scikit-learn.org/stable/index.html\"><code>scikit-learn</code></a> is a package more suited for machine learning classification, regression, and model selection as it has more sophisticated functions for unsupervised and supervised clustering of data, as well as cross-fold validation</li>\n",
    "        <li><a href=\"https://www.pymc.io/welcome.html\"><code>PyMC</code></a> is another package that is better suited for Bayesian statistical modeling and probabilistic machine learning, fitting models with Markov chain Monte Carlo (MCMC) or variational inference</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "In this notebook, we'll cover the general notation for interacting with `scipy.stats` modules in the context of the **binomial distribution**. Luckily, other probability distributions implemented in `scipy.stats` follow similar notation and parameter structure, so what you learn here is transferrable to other methods! \n",
    "\n",
    "As a refresher, the binomial distribution describes the number of \"successes\" ($k$) observed from $n$ Bernoulli trials (i.e., imagine coin flips), and the formula for the binomial **probability mass function (PMF)** can be represented as:\n",
    "\n",
    "$$ f(k) = {n \\choose k} p^k (1-p)^{n-k} $$\n",
    "\n",
    "To start off this notebook, let's learn how to model the binomial PMF and plot the values of the binomial PMF with the following parameters: $n=10$ and $p=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909031fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc694f36",
   "metadata": {},
   "source": [
    "With that preview of how to calculate the PMF for different values of $k$, it's your turn to try things out!\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Interactive Exercise #1: Plotting the Binomial Probability Mass Function</b>\n",
    "    <p></p>\n",
    "    <p>Generate a plot depicting the binomial probability mass function (PMF) for <font face=\"Latin Modern\">n = 15</font> and <font face=\"Latin Modern\">p = 0.5</font>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6922133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478967a",
   "metadata": {},
   "source": [
    "### Sampling from the Binomial Distribution ($n=15$, $p=0.5$)\n",
    "\n",
    "Sampling from a distribution lets you simulate data to understand variability, uncertainty, and the expected behavior of a process when collecting real data may be expensive, limited, or impossible. With sampling from the binomial distribution, you can imagine that we're trying to approximate theoretical results by performing repeated random coin flips with the parameters in the previous Interactive Exercise ($n=15$, $p=0.5$) with the computer rather than doing those flips ourselves!\n",
    "\n",
    "In the next code cell, we can try sampling from our distirbution 100 times and plot the results as a histogram--but lets do it multiple times and see how the distribution changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1256967",
   "metadata": {},
   "source": [
    "### The Binomial Distribution in Practice: The Palmer Penguins Dataset\n",
    "\n",
    "Now that we know how to use the `scipy.stats` module to model the binomial distribution (i.e., calculate the PMF for different values of $n$, $k$, and $p$ parameters) as well as sample from the distribution (i.e., with the `stats.binom.rvs()` function), let's dive into a practical application of probability distributions.\n",
    "\n",
    "The Palmer Penguins dataset we used in the previous lecture contains `sex` information (i.e., `male` and `female`) for our penguins! In this portion of the notebook, we will try and model the theoretical distribution of `sex` in our population and compare it to what we observe in our sample set.\n",
    "\n",
    "First, let's see how many `male` penguins we have in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec36239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020e3e6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Interactive Exercise #2: Probability of Encountering Male Penguins in the Palmer Penguins Dataset</b>\n",
    "    <p></p>\n",
    "    <p>Assuming that the probability of encountering a <code>male</code> penguin is 50%, what is the probability of observing <b>exactly this number</b> of <code>male</code> penguins within our sample? (<i>Hint</i>: You can use the <code>stats.biom.pmf()</code> function with our parameters).</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102f021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5329957",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Interactive Exercise #3: Plotting the Binomial Probability Mass Function with the Palmer Penguin Parameters</b>\n",
    "    <p></p>\n",
    "    <p>Generate a plot depicting the binomial probability mass function (PMF) for <font face=\"Latin Modern\">n = total number of penguins</font> and <font face=\"Latin Modern\">p = 0.5</font> to model the PMF for our population of penguins.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698f35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae672fc7",
   "metadata": {},
   "source": [
    "Is it surprising that we observed the number of `male` penguins that we did? How might we test this?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaa324",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2: The Central Limit Theorem\n",
    "\n",
    "As mentioned previously, the Central Limit Theorem states that for large enough sample sizes, the sampling distribution of sample means tends to be normally distributed *regardless* of the underlying probability distribution of the population. We will spend this portion of the notebook digging into this concept a little deeper!\n",
    "\n",
    "### Modeling Body Mass in the Palmer Penguins Dataset\n",
    "\n",
    "Let's assume that the entire population of penguins are the ones in the Palmer Penguins dataset, and that we are only able to obtain small samples of penguins for research purposes. For this portion of the notebook, we are going to focus on the `body_mass_g` of the penguins, so firstly, let's limit ourselves to the set of penguins for which we have a measurement of this trait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2926e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6d549",
   "metadata": {},
   "source": [
    "When we look at this plot, it doesn't really look normally distributed. But that's alright! Let's dig a little deeper and figure out what the population mean is here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee335d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84392bf9",
   "metadata": {},
   "source": [
    "Now, let's pretend that our study is not very well funded due to budget cuts. Therefore, we are only able to sample and weigh $n=10$ penguins from the population during each of our arctic expeditions. \n",
    "\n",
    "Let's determine the mean of our sample set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e19846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f92c16",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "As mentioned previously, the Central Limit Theorem states that sample means taken in this way should be normally distributed around the true population mean, and that this distribution should have a standard deviation equal to the population standard deviation.\n",
    "\n",
    "Let's test this idea out by repeating our sampling process 1,000 more times and then looking at the resulting distribution of sample means. Then, we can visualize this result as a histogram to evaluate the distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a320b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14456a98",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Interactive Exercise #3: Modeling Body Mass in the Palmer Penguins Dataset</b>\n",
    "    <p></p>\n",
    "    <p>Say we had a larger research budget, allowing us to sample 40 penguins during each expedition! Generate a plot to depict the results of sampling the body mass of penguins over 1,000 expeditions and compare it to the theoretical distribution. How does the standard error change with this increase in sampling size?</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384dbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63d81d",
   "metadata": {},
   "source": [
    "## Exercise 3: Hypothesis Testing\n",
    "\n",
    "Now that we evaluated the Central Limit Theorem in action, let's move to **hypothesis testing**! \n",
    "\n",
    "Hypothesis testing is a statistical framework for using sample data to evaluate claims about a population parameters (e.g., population mean). The Central Limit Theorem ensures that for sufficiently large samples, the sampling distribution of the test statistic is approximately Gaussian (*Normal*). This enables us to compute p-values and make probabilistic decisions even when the original data is not necessarily normally distributed.\n",
    "\n",
    "### Analyzing Body Mass Differences between Species\n",
    "\n",
    "In this section of the notebook, let's test whether the mass of `Adelie` and `Chinstrap` penguins differ. \n",
    "\n",
    "First, we can compute a z-score for the difference in the means of the two penguin species. The formula for that is:\n",
    "\n",
    "$$ z = \\frac{( \\bar{x}_1 - \\bar{x}_2 ) - ( \\mu_1 - \\mu_2 ) }{ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } } $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe128cac",
   "metadata": {},
   "source": [
    "Next, we can see what fraction of the normal distribution is to the left of this z-score and to the of this z-score multipled by `-1` to obtain the p-value. This distribution is symmetric, so technically, we can also multiply by `2` to get the proper result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1bee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5a4f8",
   "metadata": {},
   "source": [
    "With this test, we fail to reject the null hypothesis as our $p > 0.05$. \n",
    "\n",
    "Let's try this again with the Welsh Two-Sample t-Test instead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d845d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c16a8",
   "metadata": {},
   "source": [
    "The t-test statistic is very similar to the z-test statistic, therefore the p-values are also similar! t-tests are very similar to z-tests for large sample sizes ($n$). However, it is important to use t-tests instead of z-tests when sample sizes are small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1324975",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 4: Correlation Tests and Linear Models\n",
    "\n",
    "What if we want to quantify the relationship between two variables? One way to do this is with **correlation statistics**! Correlation statistics quantify the strength and direction of the association between two variables, indicating whether they tend to increase together, decrease together, or show no consistent linear relationship.\n",
    "\n",
    "The most common measure is **Pearson's correlation coefficient ($r$)**, which captures linear dependence on a scale from `-1` to `+1`. \n",
    "\n",
    "Another common metric is **Spearman's correlation ($\\rho$)**, which assesses *monotonic* relationships and is robust to non-normal measurements and outliers. (A monotonic relationship is a relationship between two variables in which one variable consistently moves in a single direction and the other consistently changes in the same or different direction; monotonic relationships can be irregular as long as the overall direction never reverses.)\n",
    "\n",
    "In this last portion of the notebook, we will be testing whether or not `bill_length_mm` is correlated with `body_mass_g`. \n",
    "\n",
    "### Evaluating the Correlation Between Bill Length and Body Mass\n",
    "\n",
    "First, let's generate a scatter plot to visualize the relationship between these two measurements.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Interactive Exercise #4: Visualizing the Relationship Between Two Quantitative Variables</b>\n",
    "    <p></p>\n",
    "    <p>Generate a scatter plot to compare the relationship between <code>bill_length_mm</code> and <code>body_mass_g</code>, differentiating the points in the plot by penguin <code>species</code>.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50301dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be45072",
   "metadata": {},
   "source": [
    "Looking at the scatterplot, it seems like there is a relationship, but this relationship is different per species! For these next few code cells, let's focus on the `Gentoo` penguins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67291ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab90ceb",
   "metadata": {},
   "source": [
    "To calculate the correlation coefficient between these two variables, we can filter our `pandas` DataFrame to the columns of interest (e.g., `bill_length_mm` and `body_mass_g`), then use the built-in `pd.DataFrame.corr()` method to calculate the correlation between the numeric columns that are present! `pd.DataFrame.corr()` also has a `method` parameter which allows you to specify the type of correlation coefficient you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21556e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4307d",
   "metadata": {},
   "source": [
    "Alternatively, you can also use the `scipy.stats` module to determine the correlation between these two variables! One of the benefits of the `scipy.stats` module is that it will not only give you the correlation value for the metric you're interested in, but also the significance (i.e., p-value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb4b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa835c24",
   "metadata": {},
   "source": [
    "From this, we can see that correlation is significant!\n",
    "\n",
    "Another way to evaluate this is to fit a **linear model** to the data. As a refresher, a linear model is a statistical framework that describes the relationship between a *response* variable and one or more *predictor* variables as a straight-line combination (i.e., linear combination) of parameters. We can do this with the `scipy.stats.linregress` functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
